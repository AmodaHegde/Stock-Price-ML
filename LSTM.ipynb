{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d9f395-1397-4bcb-a6a6-b62e56c0e67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Understanding lstm, using one feature: predicting next value of sine wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42457766-078d-410e-bb0d-bcb06e487506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary libraries\n",
    "import torch               # PyTorch library for building and training neural networks\n",
    "import torch.nn as nn      # PyTorch's neural network module for creating layers and models\n",
    "import numpy as np         # NumPy for numerical computations and data manipulation\n",
    "import matplotlib.pyplot as plt  # Matplotlib for plotting graphs to visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b877be4d-8d18-47a8-93ed-bc5fe1c919d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a sine wave dataset\n",
    "def create_sine_wave_data(seq_length, num_samples):\n",
    "    # Generate equally spaced values from 0 to num_samples\n",
    "    x = np.linspace(0, num_samples, num_samples)\n",
    "    \n",
    "    # Generate the corresponding sine values for the x values\n",
    "    y = np.sin(x)\n",
    "    \n",
    "    # 'data' will hold sequences of length 'seq_length'\n",
    "    # 'target' will hold the next value in the sine wave for each sequence\n",
    "    data = []\n",
    "    target = []\n",
    "    \n",
    "    # Create sequences of sine values and corresponding targets\n",
    "    for i in range(len(y) - seq_length):\n",
    "        data.append(y[i:i+seq_length])      # Input sequence\n",
    "        target.append(y[i+seq_length])      # Next value to predict (target)\n",
    "    \n",
    "    return np.array(data), np.array(target) # Return data and target as NumPy arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a7bb50a-aa7d-4cf7-9cc8-6fde05956cd1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.01\u001b[39m   \u001b[38;5;66;03m# Learning rate for the optimizer\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Prepare the sine wave dataset\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m data, target \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_sine_wave_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Convert the dataset into PyTorch tensors (the data type used by PyTorch models)\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# The data is reshaped to add a feature dimension, since LSTM expects input of shape (batch_size, seq_length, input_size)\u001b[39;00m\n\u001b[0;32m     13\u001b[0m data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(data, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# Adding an extra dimension for input_size\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m, in \u001b[0;36mcreate_sine_wave_data\u001b[1;34m(seq_length, num_samples)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_sine_wave_data\u001b[39m(seq_length, num_samples):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# Generate equally spaced values from 0 to num_samples\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m, num_samples, num_samples)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# Generate the corresponding sine values for the x values\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msin(x)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Define hyperparameters for the LSTM model\n",
    "seq_length = 10        # Length of input sequence (number of previous time steps used for prediction)\n",
    "num_samples = 1000     # Total number of samples in the sine wave\n",
    "hidden_size = 50       # Number of units in the LSTM's hidden layer\n",
    "num_epochs = 50        # Number of training iterations (epochs)\n",
    "learning_rate = 0.01   # Learning rate for the optimizer\n",
    "\n",
    "# Prepare the sine wave dataset\n",
    "data, target = create_sine_wave_data(seq_length, num_samples)\n",
    "\n",
    "# Convert the dataset into PyTorch tensors (the data type used by PyTorch models)\n",
    "# The data is reshaped to add a feature dimension, since LSTM expects input of shape (batch_size, seq_length, input_size)\n",
    "data = torch.tensor(data, dtype=torch.float32).unsqueeze(2)  # Adding an extra dimension for input_size\n",
    "target = torch.tensor(target, dtype=torch.float32)           # Target tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc2c962-d38a-4f44-9422-201ecf4594b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a basic LSTM model using PyTorch's nn.Module\n",
    "class BasicLSTM(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=50, output_size=1):\n",
    "        super(BasicLSTM, self).__init__()\n",
    "        \n",
    "        # Hidden size determines the number of units in the LSTM's hidden layer\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Define the LSTM layer\n",
    "        # input_size: number of input features (1 because we are passing one sine value at each time step)\n",
    "        # hidden_size: number of LSTM units\n",
    "        # batch_first=True makes sure the batch size comes first in the input shape (batch_size, seq_length, input_size)\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        \n",
    "        # Define a fully connected layer to map the hidden state to the output (next sine wave value)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state (h_0) and cell state (c_0) with zeros\n",
    "        # h_0 and c_0 have the shape (num_layers, batch_size, hidden_size)\n",
    "        # Here, num_layers = 1 since we are using a single LSTM layer\n",
    "        h_0 = torch.zeros(1, x.size(0), self.hidden_size)  # Initial hidden state\n",
    "        c_0 = torch.zeros(1, x.size(0), self.hidden_size)  # Initial cell state\n",
    "        \n",
    "        # Pass the input through the LSTM layer\n",
    "        # The output 'out' has the shape (batch_size, seq_length, hidden_size)\n",
    "        # We discard the second output, which is the new hidden and cell state, not needed in the forward pass\n",
    "        out, _ = self.lstm(x, (h_0, c_0))\n",
    "        \n",
    "        # We only care about the output of the last time step, so we select the last one using 'out[:, -1, :]'\n",
    "        # This selects the last time step for each sequence in the batch\n",
    "        out = self.fc(out[:, -1, :])  # Pass the last hidden state through the fully connected layer\n",
    "        \n",
    "        return out  # Return the predicted output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ba4d49-7749-4f28-8435-3629100885a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the LSTM model\n",
    "model = BasicLSTM(input_size=1, hidden_size=hidden_size, output_size=1)\n",
    "\n",
    "# Define the loss function as Mean Squared Error (MSE), which is typical for regression tasks\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Define the optimizer as Adam, a popular optimization algorithm, and pass the model's parameters to it\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314f6cd0-1966-476f-8e69-1397f559263f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the LSTM model\n",
    "for epoch in range(num_epochs):\n",
    "    # Set the model to training mode (this is the default mode but it's a good practice to explicitly set it)\n",
    "    model.train()\n",
    "    \n",
    "    # Forward pass: pass the input data through the model to get the predicted output\n",
    "    outputs = model(data)\n",
    "    \n",
    "    # Calculate the loss between the predicted output and the actual target values\n",
    "    loss = criterion(outputs, target)\n",
    "    \n",
    "    # Backward pass: reset the gradients to zero, perform backpropagation, and update the model's parameters\n",
    "    optimizer.zero_grad()  # Clear previous gradients\n",
    "    loss.backward()        # Perform backpropagation\n",
    "    optimizer.step()        # Update the model's parameters\n",
    "    \n",
    "    # Print the loss value every 10 epochs to monitor the training process\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cad089-3cf9-452e-9890-e84a776dc601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch the model to evaluation mode (disables certain layers like dropout, which we aren't using here)\n",
    "model.eval()\n",
    "\n",
    "# Predict the output for the entire dataset using the trained model\n",
    "# We detach the predictions from the computation graph since we don't need to track gradients in evaluation\n",
    "predicted = model(data).detach().numpy()\n",
    "\n",
    "# Plot the true sine wave and the predicted values to visually inspect the model's performance\n",
    "plt.plot(np.arange(len(target)), target, label='True Sine Wave')   # Plot the true sine wave values\n",
    "plt.plot(np.arange(len(predicted)), predicted, label='LSTM Predictions')  # Plot the predicted values\n",
    "plt.legend()  # Add a legend to differentiate between the true values and predictions\n",
    "plt.show()    # Display the plot\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
